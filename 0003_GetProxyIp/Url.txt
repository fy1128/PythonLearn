http://www.ip181.com/daili/1.html
http://www.kxdaili.com/dailiip.html
http://www.mayidaili.com/free/1










	def GetProxyIP_xicidaili(self):

		# get proxy ip from xicidaili
		def xicidaili_CaptureIp(requrl,lasttime):
			LastTime = self.TimeToINT(lasttime)
			if None == LastTime:
				return -1
			NewLastTime = None

			soup = self.CaptureWebPage(requrl)
			if None == soup:
				return -1

			try:
				trs = soup.find('table', {'id': 'ip_list'}).findAll('tr')
				for tr in trs[1:]:
					tds = tr.findAll('td')
					if None == NewLastTime:
						NewLastTime = tds[9].text.strip()
					if LastTime <= self.TimeToINT(tds[9].text.strip()):
						self.InsertIP(tds[1].text.strip(), int(tds[2].text.strip()), tds[5].text.strip())
					else:
						print 'Capture Over,New LastTime',NewLastTime
						return NewLastTime
			except Exception,e:
				print 'Expection ',e,'@ ', sys._getframe().f_code.co_filename, sys._getframe().f_code.co_name, sys._getframe().f_lineno
				return -1
			except:
				print 'Expection @ ', sys._getframe().f_code.co_filename, sys._getframe().f_code.co_name, sys._getframe().f_lineno
				return -1
			return 0

		def xicidaili_CaptureWithLoop(TotalPage,Url,LastTime):
			page = 1
			while page <= TotalPage:
				requrl = Url
				if 1 != page:
					requrl = Url + '/' + str(page)
				if 0 == xicidaili_CaptureIp(requrl,LastTime):
					print 'Get', requrl, 'Success,total page :', TotalPage
					page += 1

		urlarr = ('nn','nt','wn','wt')
		for url in urlarr:
			# check if we was first capture
			try:
				Url = 'http://www.xicidaili.com/' + url
				if 0 == self.dbcursor.execute('select LastTime from LastCaptureTime where '
												'Domain = %s',(Url,)):
					# get total page
					LastTime = '00-00-00 00:00'
					TotalPage = None
					req = urllib2.Request(Url)
					req.add_header('User-Agent',
									'Mozilla/5.0 (X11; Linux i686; rv:31.0) Gecko/20100101 Firefox/31.0 Iceweasel/31.8.0')
					try:
						soup = BeautifulSoup(urllib2.urlopen(req, timeout=5), 'lxml')
						trs = soup.find('div', {'class': 'pagination'}).findAll('a')
						if len(trs) < 3:
							return -1
						TotalPage = (int)(trs[-2].text.strip())
						trs = soup.find('table', {'id': 'ip_list'}).findAll('tr')
						tr = trs[1]
						tds = tr.findAll('td')
						LastTime = tds[9].text.strip()
					except BaseException,e:
						print e
						return -1
					except:
						print 'Expection @ ', sys._getframe().f_code.co_filename, sys._getframe().f_code.co_name, sys._getframe().f_lineno
						return -1


					# WARNING!!! IT WILL TASK A LONG LONG TIME
					xicidaili_CaptureWithLoop(5,Url,'00-00-00 00:01')

					try:
						self.dbcursor.execute('insert into LastCaptureTime (Domain,LastTime) values '
											'(%s,%s)',(Url,LastTime))
						self.dbconnect.commit()
					except BaseException,e:
						print e
						return -1

				else:
					lasttime = self.dbcursor.fetchall()
					print lasttime[0]
					xicidaili_CaptureWithLoop(10,Url,lasttime[0])

			except Exception as e:
				print e
				return -1
			except:
				print 'Expection @ ', sys._getframe().f_code.co_filename, sys._getframe().f_code.co_name, sys._getframe().f_lineno
				return -1


	def GetProxyIP_kuaidaili(self):
		def kuaidaili_TimeToINT(time):
			# time format is YY-MM-DD HH-MM
			try:
				T = time.split(' ')
				if 2 != len(T):
					return 0
				d = T[0].split('-')
				m = T[1].split(':')
				if (3 != len(d)) or (3 != len(m)):
					return 0
				return (int)(d[0]) << 20 | (int)(d[1]) << 16 | (int)(d[2]) << 11 | (int)(m[0]) << 6 | (int)(m[1])
			except:
				print 'Unknow Time', time
				return 0

		def kuaidaili_CaptureIP(requrl,lasttime):
			lt = kuaidaili_TimeToINT(lasttime)
			if 0 == lt:
				return -1
			NewLastTime = None

			soup = self.CaptureWebPage(requrl)
			if None == soup:
				return -1

			try:
				trs = soup.find('tbody').findAll('tr')
				for tr in trs:
					tds = tr.findAll('td')
					if None == NewLastTime:
						NewLastTime = tds[6].text.strip()
					if lt <= kuaidaili_TimeToINT(tds[6].text.strip()):
						self.InsertIP(tds[0].text.strip(), int(tds[1].text.strip()), tds[3].text.strip())
					else:
						print 'Capture Over,New LastTime', NewLastTime
						return NewLastTime
				return 0
			except Exception, e:
				print e
				return -1
			except:
				print 'Expection @ ', sys._getframe().f_code.co_filename, sys._getframe().f_code.co_name, sys._getframe().f_lineno
				return -1

		def kuaidaili_CaptureWithLoop(TotalPage,Url,LastTime):
			page = 1
			while page <= TotalPage:
				requrl = Url
				if 1 != page:
					requrl = Url + '/' + str(page)
				if 0 == kuaidaili_CaptureIP(requrl, LastTime):
					print 'Get', requrl, 'Success,total page :', TotalPage
					page += 1
			return

		urlarr = ('inha','intr','outha','outtr')
		for url in urlarr:
			try:
				Url = 'http://www.kuaidaili.com/free/' + url
				if 0 == self.dbcursor.execute('select LastTime from LastCaptureTime where '
				                              'Domain = %s', (Url,)):
					headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:38.0) Gecko/20100101 Firefox/38.0 Iceweasel/38.7.1'}
					req = requests.get(Url,timeout = 5,headers = headers)
					req.raise_for_status()
					soup = BeautifulSoup(req.text, 'lxml')
					trs = soup.find('div', {'id': 'listnav'}).findAll('a')
					if len(trs) < 3:
						return -1
					TotalPage = (int)(trs[-1].text.strip())
					trs = soup.find('tbody').findAll('tr')
					tds = trs[0].findAll('td')
					LastTime = tds[6].text.strip()
					self.dbcursor.execute('insert into LastCaptureTime (Domain,LastTime) values '
					                      '(%s,%s)', (Url, LastTime))
					self.dbconnect.commit()
					print 'First time to capture ', Url, 'tatal page :', TotalPage
					kuaidaili_CaptureWithLoop(5,Url,'0000-00-00 00:01:00')
				else:
					lasttime = self.dbcursor.fetchall()
					print lasttime[0]
					kuaidaili_CaptureWithLoop(10, Url, lasttime[0])
			except Exception,e:
				print e
				return -1
			except:
				print 'Expection @ ', sys._getframe().f_code.co_filename, sys._getframe().f_code.co_name, sys._getframe().f_lineno
				return -1
